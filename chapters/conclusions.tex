\chapter{Conclusiones y planes de mejora}\label{cp: conclusions}
Del siguiente trabajo se pueden extraer varias conclusiones en diferentes campos obtenidas durante su desarrollo:
\begin{itemize}
	\item Hardware \gls{AMD} Radeon empleado.
	\item Métodos de \gls{scraper}
	\item Estructuración de los metadatos en base de datos.
	\item Análisis en el dominio de la frecuencia y con redes \gls{LSTM}
\end{itemize}
\paragraph{Hardware \gls{AMD} Radeon empleado.} En ciencia de datos es muy común creer que los únicos sistemas para acelerar por hardware el entrenamiento de una red neuronal son gráficas NVIDIA mediante su lenguage CUDA. Esto no es realmente cierto, aunque la inversión de NVIDIA en este campo es mayor a sus competidores\footnote{Gráficas RTX con núcleos Tensor y la compra de Mellanox por 6900M\$ hace un año} no es la única opción. Existen centros de procesado de datos con aceleración hardware basados en Xilinx Alveo y otras \glspl{FPGA} de \gls{AWS} y su competidor más directo \gls{AMD}. Las gráficas \gls{AMD} dan rendimiento similar a las NVIDIA a menor coste. Recientemente, \gls{AMD} ha sacado \gls{ROCm} para competir con CUDA en el procesamiento de redes neuronales con TensorFlow. Su resultado es bastante aceptable aunque sigue muy por debajo de NVIDIA. No obstante, se destaca la labor de un framework de código abierto con unos drivers muy jóvenes con una grandísima capacidad de mejora. Sin duda, la competencia favorece al consumidor y este trabajo, ha sido entrenado con dicho framework dando un uso del 100\% a la gráfica \gls{AMD} Radeon 570x durante más de 48 horas seguidas. Hay que destacar que, mientras NVIDIA CUDA corre bajo varios sistemas operativos, \gls{ROCm} lo hace sólo bajo Kernel Linux\cite{rocm}.

\paragraph{Métodos de \gls{scraper}.} La capacidad de estos métodos ha quedado más que demostrada con la cantidad de datos que se es capaz de generar con unas líneas de código. Aquí aparece de nuevo un debate ético entre, qué se debe y qué no se debe hacer. Estas herramientas permiten la descarga masiva de datos de un servidor y, a la vez que son una herramienta para desarrolladores, también lo son para hackers y software malicioso.

\paragraph{Estructuración de los metadatos en base de datos.} Este sin lugar a dudas ha sido uno de los puntos clave del trabajo. La evolución de un trabajo con datos es altamente dependiente de los datos de los cuales se parta y de su organización. Una correcta estructuración de los datos conlleva una inversión de tiempo y esfuerzo que se ve recompensada con la reducción de problemas a la hora de explotarlos, lo que implica la mayoría de las veces repetir trabajo. El almacenamiento de los metadatos en la base datos ha permitido, hacer una estimación del tamaño de la base de datos, sacar un relación de audios compatibles con la tasa de muestreo del modelo (sin llegar a obtener un error o, lo que es peor, procesar datos de manera errónea) y tener una monitorización de la procedencia de los mismos.

\paragraph{Análisis en el dominio de la frecuencia y con redes \gls{LSTM}}
A pesar de la gran cantidad de variaciones que se han hecho sobre la red, no se ha conseguido llegar a unos buenos resultados. El trabajo en el dominio de la frecuencia es complejo y el tuneo de las celdas \gls{LSTM} es una tarea complicada. A pesar de ello, se han barajado muchas posibilidad y se ha llegado a un alto grado de comprensión de las redes recurrentes. Pero, resolver un problema tan complejo con la eliminación de ruido en el dominio de la frecuencia no es una tarea fácil. Por tanto, este trabajo presenta las bases sobre las que empezar a desarrollar un abanico de redes para intentar resolver el problema.

\paragraph{Propuestas de mejora}
Dado que los resultados no son los esperados se proponen una serie de mejoras para intentar mejorar el algoritmo.
\begin{itemize}
	\item \textbf{Crear un mecanismo de detección activa de voz}. Con este mecanismo automáticamente se elimina todo el espectro, o se le aplica una ganancia negativa muy alta, a las partes en las que no se detecte voz. De esta manera se elimina mucho ruido sin necesidad de llegar a la red neuronal.
	\item \textbf{Detección de la frecuencia principal y sus armónicos (pitch frecuency)}. Detectar la frecuencia fundamental y sus armónicos y pre-limpiar el espectro a la entrada del algoritmo.
	\item \textbf{Trabajar sobre la normalización de los datos}. Este es uno de los puntos con mayor influencia sobre los resultados. Los resultados de la red variaban mucho con datos normalizados o sin normalización. Se debe plantear una normalización que sea posible aplicarla en tiempo real, es decir sólo se conoce el valor de las muestras actuales y las pasadas, no las futuras como en entrenamiento.
	\item \textbf{Entrenar el último modelo con mayor número de datos}. Dados los resultados obtenidos, el primer punto a tratar sería entrenar el modelo de mayor precisión con más datos ayudándose del generador de secuencias.
\end{itemize}

\section{Cierre y agradecimientos}
Este trabajo ha hecho uso de librerías de código abierto así como de publicaciones científicas, frameworks de acceso público y el temario aprendido durante el curso. El desarrollo de código se genera en su totalidad en Python 3.7 con las librerías:
\begin{itemize}
	\item numpy
	\item sqlite3
	\item logging
	\item os
	\item socket
	\item getpass
	\item time
	\item random
	\item selenium
	\item beautifulsoup
	\item requests
	\item tqdm
	\item audioread
	\item copy
	\item zipfile
	\item youtube\_dl
	\item argparse
	\item scipy
	\item librosa
	\item pydub
	\item h5py
	\item tensorflow
	\item matplotlib
\end{itemize}

El presente documento ha sido desarrollado con \LaTeX y compilado con \XeLaTeX. Todos los gráficos aquí presentes se han desarrollado en Tikz para este documento salvo lo obtenidos de la bibliografía \cite{rnnoise} que son importados desde la publicación.

Con todo esto, se agradece a la comunidad \textbf{open-source} dado que sin sus aportes este trabajo no sería posible.